{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "\n",
    "from dgp import get_dataloader\n",
    "from utils.default_device import get_default_device\n",
    "from utils.llc import calculate_llc_for_file, evaluate_fn\n",
    "from utils.loading import Conf, load_model_from_hf\n",
    "import torch.nn.functional as F\n",
    "from utils import move_to_device\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# device = \"cpu\"\n",
    "device = get_default_device()\n",
    "\n",
    "hf_repo_name = \"cybershiptrooper/ConceptPerlocation_ckpts_98k\"\n",
    "dump_dir = f\"results/scratch/{hf_repo_name}/llc\"\n",
    "model_dir = f\"results/scratch/{hf_repo_name}\"\n",
    "\n",
    "\n",
    "os.makedirs(dump_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "model = load_model_from_hf(0, hf_repo_name, epoch=0)\n",
    "config = Conf(**model[\"config\"])\n",
    "\n",
    "config.device = device\n",
    "\n",
    "dataloader = get_dataloader(\n",
    "    n_relative_properties=config.data.n_relative_properties,\n",
    "    n_descriptive_properties=config.data.n_descriptive_properties,\n",
    "    n_descriptive_values=config.data.n_descriptive_values,\n",
    "    num_of_classes_to_divide_over=config.data.num_of_classes_to_divide_over,\n",
    "    prior_param=config.data.prior_param,\n",
    "    props_prior_type=config.data.props_prior_type,\n",
    "    n_entities=config.data.n_entities,\n",
    "    instr_ratio=config.data.instr_ratio,\n",
    "    max_sample_length=config.data.max_sample_length,\n",
    "    num_iters=5e5 * config.data.batch_size,\n",
    "    batch_size=config.data.batch_size,\n",
    "    num_workers=config.data.num_workers,\n",
    "    seed=config.seed,\n",
    ")\n",
    "pad_token_id = dataloader.dataset.pad_token_id\n",
    "\n",
    "all_inputs = []\n",
    "all_labels = []\n",
    "all_masks = []\n",
    "for i, batch in tqdm(enumerate(dataloader)):\n",
    "    if i > 100:\n",
    "        break\n",
    "    sequences, symb_sequences, seq_lengths, seq_logprobs, _ = batch\n",
    "    inputs = sequences[:, :-1]\n",
    "    labels = sequences[:, 1:].clone()\n",
    "    # procesed_batch = [inputs, labels]\n",
    "    all_inputs.append(inputs)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "    mask = (labels != pad_token_id).float()\n",
    "    all_masks.append(mask)\n",
    "\n",
    "# make a dataset with all the inputs and labels\n",
    "print(\"Making a new dataloader with all the data\")\n",
    "all_inputs = torch.cat(all_inputs, dim=0)\n",
    "all_labels = torch.cat(all_labels, dim=0)\n",
    "all_masks = torch.cat(all_masks, dim=0)\n",
    "print(all_inputs.shape, all_labels.shape)\n",
    "print(all_inputs.device, all_labels.device)\n",
    "new_dataloader = DataLoader(\n",
    "    TensorDataset(all_inputs, all_labels, all_masks),\n",
    "    batch_size=config.data.batch_size,\n",
    "    num_workers=config.data.num_workers,\n",
    "    shuffle=True,\n",
    ")\n",
    "print(\"Done making the new dataloader\")\n",
    "\n",
    "\n",
    "def evaluate_fn_new(model, data):\n",
    "    inputs, labels, mask = data\n",
    "    logits = model(inputs)  # (B, L-1, V)\n",
    "    loss = F.cross_entropy(\n",
    "        logits.transpose(1, 2),\n",
    "        labels,\n",
    "        reduction=\"none\",\n",
    "    )  # (B*L-1)\n",
    "    loss = (loss * mask).sum() / mask.sum()\n",
    "    return loss, {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "evaluator = evaluate_fn_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "llc_outputs = []\n",
    "iters = (\n",
    "    list(range(0, 200, 10))\n",
    "    + list(range(200, 1200, 20))\n",
    "    + list(range(1200, 5_001, 50))\n",
    "    + list(range(5_100, 10_001, 100))\n",
    "    + list(range(10_500, 25_501, 500))\n",
    "    + list(range(26_000, 100_501, 1000))\n",
    ")\n",
    "for iter in tqdm(iters):\n",
    "    print(f\"Calculating LLC for iteration {iter}\")\n",
    "    time_now = time.time()\n",
    "    llc_output = calculate_llc_for_file(\n",
    "        iter,\n",
    "        new_dataloader,\n",
    "        model_dir=hf_repo_name,\n",
    "        config=config,\n",
    "        evaluate_fn=evaluator,\n",
    "        model_loader=load_model_from_hf,\n",
    "        num_chains=5,\n",
    "        num_draws=200,\n",
    "        vocab_size=dataloader.dataset.PCSG.vocab_size,\n",
    "    )\n",
    "    time_taken = time.time() - time_now\n",
    "    # print formatted time\n",
    "    print(f\"Time taken: {time_taken//60:.0f}m {time_taken%60:.0f}s\")\n",
    "    llc_outputs.append(llc_output)\n",
    "    with open(f\"{dump_dir}/llc_output_it_{iter}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(llc_output, f)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
